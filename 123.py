def __init__(self, data_name, raw_data, tokenizer, bot, eot):
    super(SupervisedDataset, self).__init__()
    self.tokenizer = tokenizer
    self.data_name = data_name
    questions, cots, answers = [], [], []
    group_keys = []

    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total": len(raw_data),
        "processed": 0,
        "skipped_bad_data": 0,
        "skipped_too_long": 0,
        "skipped_invalid_answer": 0,
    }

    print(f"\n{'='*60}")
    print(f"ğŸ“Š Processing {self.data_name} dataset")
    print(f"{'='*60}")

    # ä½¿ç”¨ tqdm åŒ…è£… enumerateï¼Œæ˜¾ç¤ºæ¸…æ™°çš„è¿›åº¦æ¡
    with tqdm(
            total=len(raw_data),
            desc="ğŸ”„ Parsing examples",
            unit="example",
            ncols=100,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
    ) as pbar:
        for num_iter, example in enumerate(raw_data):
            # å®éªŒæ¨¡å¼ï¼šé™åˆ¶æ•°æ®é‡
            if training_args.exp_mode and num_iter >= training_args.exp_data_num:
                pbar.update(len(raw_data) - num_iter)  # è·³è¿‡å‰©ä½™
                break

            q_key = str(example.get("question_id", example["question"])).strip()
            question = f"{example['question']}"

            skip_reason = None

            try:
                if "icot" in self.data_name and "full" in self.data_name:
                    # icot-full (GSM8k-Aug-NL)
                    if example["answer"] is None:
                        skip_reason = "bad_data"
                    else:
                        token_num = len(tokenizer.encode(
                            example["question"] + example["cot"] + example["answer"]
                        ))
                        if token_num > training_args.max_token_num:
                            skip_reason = "too_long"
                        else:
                            cot = f"{example['cot']}".split(". ")
                            if not training_args.include_last_cot:
                                cot = cot[:-1]

                            answer = example["answer"].split(" ")[-1]
                            if not answer[0].isdigit():
                                skip_reason = "invalid_answer"
                            else:
                                answer = f"The answer is: {answer}"
                                answer = answer.replace("####", "")
                                questions.append(question)
                                cots.append(". ".join(cot) + ".\n" if cot else "")
                                answers.append(answer)
                                group_keys.append(q_key)
                                stats["processed"] += 1

                elif "icot" in self.data_name:
                    # icot (GSM8k-Aug)
                    token_num = len(tokenizer.encode(
                        example["question"] + example["cot"] + example["answer"]
                    ))
                    if token_num > training_args.max_token_num:
                        skip_reason = "too_long"
                    else:
                        cot = f"{example['cot']}".split(" ")
                        if not training_args.include_last_cot:
                            cot = cot[:-1]

                        answer = example["answer"].split(" ")[-1]
                        if not answer[0].isdigit():
                            skip_reason = "invalid_answer"
                        else:
                            answer = f"The answer is: {answer}"
                            answer = answer.replace("####", "")
                            questions.append(question)
                            cots.append(" ".join(cot))
                            answers.append(answer)
                            group_keys.append(q_key)
                            stats["processed"] += 1

                elif "commonsense" in self.data_name or "strategy" in self.data_name:
                    question = example["question"].strip() + "\n"
                    cot = example["cot"].strip() + "\n"
                    answer = f"The answer is: {str(example['answer']).strip()}"

                    token_num = len(tokenizer.encode(question + " " + cot + " " + answer))
                    if token_num > training_args.max_token_num:
                        skip_reason = "too_long"
                    else:
                        questions.append(question)
                        cots.append(cot)
                        answers.append(answer)
                        group_keys.append(q_key)
                        stats["processed"] += 1

                elif "prontoqa" in data_args.data_name:
                    question = example["question"].strip() + "\n"
                    cot = "\n".join(example["steps"][:-1]) + "\n"
                    answer = f"The answer is: {str(example['answer']).strip()}"

                    token_num = len(tokenizer.encode(question + " " + cot + " " + answer))
                    if token_num > training_args.max_token_num:
                        skip_reason = "too_long"
                    else:
                        questions.append(question)
                        cots.append(cot)
                        answers.append(answer)
                        group_keys.append(q_key)
                        stats["processed"] += 1

                else:
                    raise NotImplementedError

            except Exception as e:
                skip_reason = "error"
                pbar.set_postfix_str(f"âš ï¸  Error: {str(e)[:30]}")

            # æ›´æ–°è·³è¿‡ç»Ÿè®¡
            if skip_reason:
                if skip_reason == "bad_data":
                    stats["skipped_bad_data"] += 1
                elif skip_reason == "too_long":
                    stats["skipped_too_long"] += 1
                elif skip_reason == "invalid_answer":
                    stats["skipped_invalid_answer"] += 1

            # æ›´æ–°è¿›åº¦æ¡çš„åç¼€ä¿¡æ¯ï¼ˆæ˜¾ç¤ºå®æ—¶ç»Ÿè®¡ï¼‰
            pbar.set_postfix_str(
                f"âœ“ {stats['processed']} | "
                f"â­ï¸  {stats['skipped_bad_data']+stats['skipped_too_long']+stats['skipped_invalid_answer']}"
            )
            pbar.update(1)

    # æ‰“å°å¤„ç†æ‘˜è¦
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ Processing Summary:")
    print(f"{'='*60}")
    print(f"  Total examples:           {stats['total']:>6}")
    print(f"  âœ“ Processed:              {stats['processed']:>6} ({stats['processed']/stats['total']*100:.1f}%)")
    print(f"  â­ï¸  Skipped (bad data):     {stats['skipped_bad_data']:>6}")
    print(f"  â­ï¸  Skipped (too long):     {stats['skipped_too_long']:>6}")
    print(f"  â­ï¸  Skipped (invalid ans):  {stats['skipped_invalid_answer']:>6}")
    print(f"{'='*60}\n")

    # å®éªŒæ¨¡å¼æˆªæ–­
    if training_args.exp_mode:
        questions = questions[:training_args.exp_data_num]
        cots = cots[:training_args.exp_data_num]
        answers = answers[:training_args.exp_data_num]
        group_keys = group_keys[:training_args.exp_data_num]
        print(f"âš™ï¸  Experiment mode: Limited to {len(questions)} samples\n")

    # å¤šæ ·æœ¬æ‰©å±•ï¼ˆç”¨äºå¤šå›ç­”å¯¹æ¯”å­¦ä¹ ï¼‰
    K = getattr(training_args, "samples_per_group", 1)
    if K > 1:
        print(f"ğŸ”„ Expanding samples: {len(questions)} â†’ {len(questions) * K} (K={K} per group)")
        questions = sum([[q] * K for q in questions], [])
        cots = sum([[c] * K for c in cots], [])
        answers = sum([[a] * K for a in answers], [])
        group_keys = sum([[g] * K for g in group_keys], [])

    print(f"ğŸ“¦ Final dataset size: {len(questions)} samples")
    print(f"ğŸ·ï¸  Unique groups: {len(set(group_keys))}\n")

    # Tokenizationï¼ˆè°ƒç”¨ preprocessï¼Œå†…éƒ¨ä¹Ÿæœ‰è¿›åº¦æ¡ï¼‰
    self.data_dict = preprocess(questions, cots, answers, tokenizer, bot, eot)
    self.keys = list(self.data_dict.keys())

    # ç”Ÿæˆ group_ids
    self.group_ids = torch.tensor(
        [_stable_hash(k) for k in group_keys], dtype=torch.long
    )
    self.keys.append("group_ids")
    self.data_dict["group_ids"] = self.group_ids

    print(f"âœ… Dataset ready: {len(self)} samples, {len(set(self.group_ids.tolist()))} groups\n")
